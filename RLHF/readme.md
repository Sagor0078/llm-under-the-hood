### Reinforcement Learning from Human Feedback (RLHF), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO)

These techniques are at the forefront of aligning Large Language Models (LLMs) with human preferences and improving their performance on complex tasks. They build upon the principles of Reinforcement Learning (RL) to refine model behavior.

- [Reinforcement Learning from Human Feedback explained with math derivations and the PyTorch code by Omar Jamil](https://www.youtube.com/watch?v=qGyFrqc34yc&t=300s)
- [Group Relative Policy Optimization (GRPO)](https://www.youtube.com/watch?v=Yi1UCrAsf4o)
- [DeepSeek R1 Theory Overview | GRPO + RL + SFT](https://www.youtube.com/watch?v=QdEuh2UVbu0)
- [Reinforcement Learning from Human Feedback (RLHF), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO)](https://g.co/gemini/share/789b35d3de43)
- [Proximal Policy Optimization Algorithms" by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov (2017)](https://arxiv.org/abs/1707.06347)
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" by Shao et al. (2024)](https://arxiv.org/abs/2402.03300)