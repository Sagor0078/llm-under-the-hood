## Flash Attention, an I/O-aware algorithm that significantly speeds up the attention mechanism in transformer models by optimizing memory usage on modern GPUs


- [When the Math Must Obey the GPU! || Speeding Up Modern Deep Learning](https://www.youtube.com/watch?v=3YZ6r-0Cw5k)
- [flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://towardsdatascience.com/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive-724af489997b/&ved=2ahUKEwjNgMPlud2OAxUUdvUHHY6JHmsQFnoECDAQAQ&usg=AOvVaw2Z_ljeN4zqUfeYtStliiG8)