

# LLM Under The Hood â€“ Directory Overview

| Module/Folder         | Files Included                                     | Description                                |
|-----------------------|----------------------------------------------------|--------------------------------------------|
| [Attention_Mechanism](https://github.com/Sagor0078/llm-under-the-hood/tree/main/Attention_Mechanism) | `main.py`, `readme.md`                             | Code and explanation of attention mechanisms |
| [KV_Cache](https://github.com/Sagor0078/llm-under-the-hood/tree/main/KV_Cache)            | `main.py`, `readme.md`                             | Key-Value caching in Transformers          |
| [LICENSE](https://github.com/Sagor0078/llm-under-the-hood/blob/main/LICENSE)             | `LICENSE`                                          | License file for the repository            |
| [optimizing_llm_inference](https://github.com/Sagor0078/llm-under-the-hood/tree/main/optimizing_llm_inference) | *(File or folder not expanded)*               | Optimization strategies (details unknown)  |
| [PromptTechnique](https://github.com/Sagor0078/llm-under-the-hood/tree/main/PromptTechnique)     | `main.py`, `readme.md`                             | Prompt engineering techniques              |
| [Quantization](https://github.com/Sagor0078/llm-under-the-hood/tree/main/Quantization)        | `main.py`, `readme.md`                             | Model quantization techniques              |
| [README.md](https://github.com/Sagor0078/llm-under-the-hood/blob/main/README.md)           | `README.md`                                        | Main project overview                      |
| [RLHF](https://github.com/Sagor0078/llm-under-the-hood/tree/main/RLHF)                | `GRPO.py`, `PPO.py`, `readme.md`                   | Reinforcement Learning from Human Feedback |
| [RMSNorm](https://github.com/Sagor0078/llm-under-the-hood/tree/main/RMSNorm)             | `main.py`, `readme.md`, images                     | RMS Normalization with visuals             |
| [RoPE](https://github.com/Sagor0078/llm-under-the-hood/tree/main/RoPE)                | `main.py`, `readme.md`, `rope.png`                | Rotary Positional Embedding                |
| [SwiGLU](https://github.com/Sagor0078/llm-under-the-hood/tree/main/SwiGLU)              | `main.py`, `readme.md`                             | SwiGLU activation function                 |
| [Tokenizer](https://github.com/Sagor0078/llm-under-the-hood/tree/main/Tokenizer)         | `main.py`, `readme.md`                             | Tokenization process in LLMs               |


## External Learning Resources

| Resource | Link | Description |
|---------|------|-------------|
| Stanford CS336 â€“ LLMs From Scratch | [ðŸ”— CS336 Course Page](https://web.stanford.edu/class/cs336/) | Advanced graduate course on large language models from scratch |
| Umar Jamilâ€™s LLM Tutorial Series | [YouTube Playlist](https://www.youtube.com/@umarjamilai/featured) | Step-by-step LLM implementation tutorials |
| Andrej Karpathy â€“ GPT From Scratch | [YouTube Video](https://www.youtube.com/watch?v=kCc8FmEb1nY) | Karpathy's famous transformer explainer video |

